# Experiment 2 Linear Regression

## Perceptron

+ Given a set of $n$ data points and their corresponding responses  $D=\left\{\boldsymbol{x}_{i}, y_{i}\right\}^{n}_{i=1}$ 
+ we try to find a linear function   $\boldsymbol{f}(\boldsymbol{x};\boldsymbol{w}, b) = \boldsymbol{w}^\boldsymbol{T} \boldsymbol{x} + b$   to optimize
  + $\begin{aligned} \min _{\{\boldsymbol{w}, b\}} \max & \frac{1}{2 n} \sum_{i=1}^{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b-y_{i}\right)^{2}  +\alpha\left(\boldsymbol{w}^{T} \boldsymbol{w}-1\right) \end{aligned}$
  + Update the weights:
    + $\boldsymbol{w}^{t+1}=\boldsymbol{w}^{t}+\left(y_{i}-p_{i}\right) \boldsymbol{x}_{i}, b^{t+1}=b^{t}+\left(y_{i}-p_{i}\right)$
+ Let $\boldsymbol{z}=\left(\begin{array}{l}\boldsymbol{W} \\ b\end{array}\right), \boldsymbol{y}=\left(y_{1}, \ldots, y_{n}\right)^{T}, \boldsymbol{A}=\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n}, \mathbf{1}\right)^{T}$ , where **1** is a vector with all ones. Hence
  + $\sum_{i=1}^{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b-y_{i}\right)^{2}=\|\boldsymbol{A} \boldsymbol{z}-\boldsymbol{y}\|_{2}^{2}, \boldsymbol{w}^{T} \boldsymbol{w}-1=\boldsymbol{z}^{T} \boldsymbol{z}-b^{2}-1$

## Experiment 

+ Download a regression dataset from UCI machine learning repository ([https://](https://archive.ics.uci.edu/ml/datasets.php)[archive.ics.uci.edu/ml/datasets.php](https://archive.ics.uci.edu/ml/datasets.php)).  
+ ä½¿ç”¨çš„æ•°æ®é›†æè¿°å¦‚ä¸‹

> **Data Set Information:**
>
> These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.
>
> I think that the initial data set had around 30 variables, but for some reason I only have the 13 dimensional version. I had a list of what the 30 or so variables were, but a.) I lost it, and b.), I would not know which 13 variables are included in the set.
>
> The attributes are (dontated by Riccardo Leardi, riclea **'@'** anchem.unige.it )
> 1) Alcohol
> 2) Malic acid
> 3) Ash
> 4) Alcalinity of ash
> 5) Magnesium
> 6) Total phenols
> 7) Flavanoids
> 8) Nonflavanoid phenols
> 9) Proanthocyanins
> 10)Color intensity
> 11)Hue
> 12)OD280/OD315 of diluted wines
> 13)Proline
>
> In a classification context, this is a well posed problem with "well behaved" class structures. A good data set for first testing of a new classifier, but not very challenging.

+ å¯¹æ•°æ®è¿›è¡Œå¤„ç†

```python
import pandas as pd
import random 
import numpy as np
import matplotlib.pyplot as plt

N = 11

# è¯»å–æ•°æ®
data_pd = pd.read_csv('winequality-red.csv', header=0, sep=';')
data = []
# æŠŠè¾“å‡ºåˆ†å‰²å‡ºæ¥
for _, row in data_pd.iterrows():
    data.append([[row['fixed acidity'], row['volatile acidity'], row['citric acid'],
              row['residual sugar'], row['chlorides'], row['free sulfur dioxide'],
              row['total sulfur dioxide'], row['density'], row['pH'],
              row['sulphates'], row['alcohol']],row['quality']])


# æ‰“ä¹±é¡ºåºè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•
random.shuffle(data)

# åˆ’åˆ†è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
count = len(data)
ratio = 0.8
split_point = int(count * ratio)
tran_data = data[:split_point]
test_data = data[split_point:]
print('There {} datas in total, {} datas used for train, {} used for test'.format(count, split_point, count - split_point))

# There 1599 datas in total, 1279 datas used for train, 320 used for test
```

+ æ ·æœ¬æ•°æ®çš„å­˜å‚¨ç»“æ„ä¸º $data_x : [[x_1,x_2,\dots],label]$ 

+ ç„¶åå®ç°LinearRegressionï¼Œå°†å…¶å°è£…æˆç±»

  + é¦–å…ˆå¯¹é¢„æµ‹å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œè®¾ç½®å­¦ä¹ ç‡ 

    + ```python
      def __init__(self):
          super(LinearRegression, self).__init__()
          # åˆå§‹åŒ–å‚æ•°
          self.dim = N
          self.w = [0 for i in range(self.dim)]
          self.b = 0
          self.learningRate = 0.00005
        self.alpha = 1
      ```

  + ç„¶åå®ç°æ­£å‘ä¼ æ’­å’Œåå‘ä¼˜åŒ–

    + Calculate the activation output:

      + $p_{i}=f\left(\boldsymbol{w}^{t} \cdot \boldsymbol{x}_{i}+b\right)$ 

    + Update the weights:

      + $\boldsymbol{w}^{t+1}=\boldsymbol{w}^{t}+\left(y_{i}-p_{i}\right) \boldsymbol{x}_{i}, b^{t+1}=b^{t}+\left(y_{i}-p_{i}\right)$

    + Compute error
  
      + $\begin{aligned} \min _{\{\boldsymbol{w}, b\}} \max & \frac{1}{2 n} \sum_{i=1}^{n}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b-y_{i}\right)^{2}  +\alpha\left(\boldsymbol{w}^{T} \boldsymbol{w}-1\right) \end{aligned}$
  
    + æ ¹æ®ä¸Šè¿°å…¬å¼ï¼Œè¿›è¡Œä»£ç å®ç°
  
    + ```python
      def response(self, x):
          """è®¡ç®—é¢„æµ‹ç»“æœï¼šæ±‚å’Œï¼Œæ¿€æ´»"""
          y = sum([i * j for i, j in zip(self.w, x)]) + self.b
          if y >= 0:
              return 1
          else:
              return 0
      
      def updateWeights(self, x, iterError):
          """
              æ›´æ–°å‚æ•°æƒé‡
              w(t+1) = w(t) + (yi - pi) * xi
            b(t+1) = b(t) + (yi - pi)
          """
        # self.w += self.learningRate * iterError * int(x)
          self.w = [i + self.learningRate * iterError * j for i, j in zip(self.w, x)]
          self.b += self.learningRate * iterError
      ```
  
  + åœ¨ç½‘ç»œè®­ç»ƒéƒ¨åˆ†ï¼Œå¯¹æ¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè¿›è¡Œé¢„æµ‹ï¼Œå¦‚æœé¢„æµ‹é”™è¯¯ï¼Œåˆ™è¿›è¡Œæƒé‡æ›´æ–°ã€‚ç›´åˆ°æœ¬æ¬¡epochï¼Œæ‰€æœ‰é¢„æµ‹ç»“æœéƒ½å‡†ç¡®
  
    + ```python
      def response(self, x):
          """
          è®¡ç®—é¢„æµ‹ç»“æœï¼šæ±‚å’Œï¼Œæ¿€æ´»
          ğ’‡(ğ’™;ğ’˜,ğ‘)=ğ’˜^ğ‘» ğ’™+ğ‘ 
          """
          y = sum([i * j for i, j in zip(self.w, x)]) + self.b
          return y
      
      def updateWeights(self, x, iterError):
          """
          æ›´æ–°å‚æ•°æƒé‡
          w(t+1) = w(t) + (yi - pi) * xi
          b(t+1) = b(t) + (yi - pi)
          """
          # self.w += self.learningRate * iterError * int(x)
          self.w = [ i + self.learningRate * iterError * j for i, j in zip(self.w, x)]
          self.b += self.learningRate * iterError
        self.alpha -= self.learningRate * iterError
      
      def computeError(self, x, y, r):
          """
          è®¡ç®—æŸå¤±
          minâ”¬{ğ’˜,ğ‘}   (mğ‘ğ‘¥)â”¬ğ›¼  1/2ğ‘› âˆ‘_(ğ‘–=1)^ğ‘› ((ğ’˜^ğ‘‡ ğ’™_ğ‘–+ğ‘âˆ’ğ‘¦_ğ‘–))^2 +ğ›¼(ğ’˜^ğ‘‡ ğ’˜âˆ’1).
        Let ğ’›=((ğ’˜ ğ‘)), ğ’š=((ğ‘¦_1,â€¦,ğ‘¦_ğ‘›))^ğ‘‡, ğ‘¨=((ğ’™_1,â€¦,ğ’™_ğ‘›,ğŸ))^ğ‘», 
          where ğŸ is a vector with all ones. Hence,
          âˆ‘_(ğ‘–=1)^ğ‘› ((ğ’˜^ğ‘‡ ğ’™_ğ‘–+ğ‘âˆ’ğ‘¦_ğ‘–))^2 = ||ğ‘¨ğ’›âˆ’ğ’š||_2^2, ğ’˜^ğ‘‡ ğ’˜âˆ’1=ğ’›^ğ‘‡ ğ’›âˆ’ğ‘^2âˆ’1. 
          """
          z = self.w + [self.b]
          A = x + [1]
          error1 = error2 = 0
          error1 = 1/10 * (sum([i * j for i, j in zip(A, z)]) - y) ** 2
          error2 = sum([i * j for i, j in zip(z, z)]) - self.b * self.b - 1
        return error1 + self.alpha * error2
      ```
      
    + ```python
      # ç½‘ç»œè®­ç»ƒ
      Net = LinearRegression()
      Net.train(tran_data)
      
      # Epoch 1980 finished, globalError is 72.82630575237867
      # Epoch 1981 finished, globalError is 71.77918532208149
      # Epoch 1982 finished, globalError is 70.54836659086041
      # Epoch 1983 finished, globalError is 68.97983245871387
      # end training
      ```
      
    + ```python
    # å¯è§†åŒ–
      print('é¢„è®¾åˆ†å‰²çº¿ï¼šy =',w,'x +',b)
    print('é¢„æµ‹åˆ†å‰²çº¿ï¼š', p.w[0],'* x + ', p.w[1],'* y =', - p.b)
      
    for data in data_x:
          if data[1] == 0:
              plt.plot(data[0][0],data[0][1],'g^')
          else:
              plt.plot(data[0][0],data[0][1],'ro')
      
      plt.plot([-7,7],[-7*w+b,7*w+b]) # é¢„è®¾åˆ†å‰²çº¿
      plt.plot([-7,7],[-(-7.0*p.w[0]+p.b)/p.w[1],-(7.0*p.w[0]+p.b)/p.w[1]]) # é¢„æµ‹åˆ†å‰²çº¿
      plt.grid(True)
      ```
  
  + æœ€åè¿›è¡Œæµ‹è¯•
  
    + ```python
      # æµ‹è¯•éƒ¨åˆ†
      total_error = 0
      for data in test_data:
          r = p.response(data[0])
          total_error += abs(data[1] - r)
      print(f'test data number: {len(test_data)} , total test loss is {total_error}')
      
      # test data number: 320 , total test loss is 155.0047567921725
      ```
      
